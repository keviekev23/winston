{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "winston_whisper_finetune.ipynb",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Winston Flywheel — Whisper Fine-Tuning\n**Run this notebook in Google Colab after uploading training data with `scripts/upload_training_data.py`.**\n\nThis notebook:\n1. Loads kitchen speech audio from your Google Drive\n2. Generates high-quality pseudo-labels using Whisper Large-v3\n3. Fine-tunes Whisper Small.en with QLoRA on utterances where Small was wrong\n4. Saves the merged model back to Drive for local MLX conversion\n\n**Estimated runtime:** 30–90 minutes depending on dataset size and GPU assigned.  \n**Required GPU:** T4 or better (Colab free tier works; Pro is faster).\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# ── Step 1: Verify GPU and mount Google Drive ──────────────────────────────\nimport subprocess\nresult = subprocess.run([\"nvidia-smi\", \"--query-gpu=name,memory.total\", \"--format=csv,noheader\"],\n                        capture_output=True, text=True)\nprint(\"GPU:\", result.stdout.strip() or \"NOT FOUND — go to Runtime → Change runtime type → T4 GPU\")\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\nprint(\"Drive mounted ✓\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# ── Step 2: Install dependencies ───────────────────────────────────────────\n# This takes ~2 minutes on first run. Output is suppressed for clarity.\nimport subprocess, sys\n\npackages = [\n    \"transformers>=4.40\",\n    \"peft>=0.10\",\n    \"datasets>=2.18\",\n    \"accelerate>=0.29\",\n    \"evaluate\",\n    \"jiwer\",\n    \"librosa\",\n    \"soundfile\",\n    \"bitsandbytes\",  # for 8-bit loading\n]\n\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + packages, check=True)\nprint(\"Dependencies installed ✓\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## ⚙️ Configuration\n**Edit the cell below.** Everything else in the notebook uses these values.\n\n- `DRIVE_DATA_DIR`: where your uploaded WAVs live (from `upload_training_data.py`)\n- `DRIVE_MODELS_DIR`: where the trained model will be saved\n- `CYCLE_NAME`: increment this each run so you don't overwrite previous adapters\n- `CONFIDENCE_THRESHOLD`: utterances below this from Whisper Small are training candidates\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# ══════════════════════════════════════════════════\n# ⚙️  EDIT THIS CELL — all config lives here\n# ══════════════════════════════════════════════════\n\nDRIVE_DATA_DIR   = \"/content/drive/MyDrive/winston-flywheel/data\"\nDRIVE_MODELS_DIR = \"/content/drive/MyDrive/winston-flywheel/models\"\n\nCYCLE_NAME = \"cycle-1\"       # ← increment each training run\n\nWHISPER_LARGE   = \"openai/whisper-large-v3\"\nWHISPER_SMALL   = \"openai/whisper-small.en\"\n\nCONFIDENCE_THRESHOLD = 0.7   # utterances below this are training candidates\nVALIDATION_SPLIT     = 0.15  # 15% held out for validation\nMAX_AUDIO_SECS       = 30    # skip utterances longer than this\n\n# Training hyperparameters — reasonable defaults for a small kitchen dataset\nLORA_R           = 8\nLORA_ALPHA       = 32\nLEARNING_RATE    = 1e-3\nTRAIN_EPOCHS     = 10\nBATCH_SIZE       = 8         # reduce to 4 if you get OOM errors\nGRAD_ACCUM_STEPS = 2\n\n# ══════════════════════════════════════════════════\nimport os\nos.makedirs(f\"{DRIVE_MODELS_DIR}/merged/{CYCLE_NAME}\", exist_ok=True)\nos.makedirs(f\"{DRIVE_MODELS_DIR}/checkpoints/{CYCLE_NAME}\", exist_ok=True)\nprint(f\"Cycle: {CYCLE_NAME}\")\nprint(f\"Data:  {DRIVE_DATA_DIR}/audio\")\nprint(f\"Output: {DRIVE_MODELS_DIR}/merged/{CYCLE_NAME}\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# ── Step 3: Load audio data from Drive ──────────────────────────────────────\nimport json\nimport glob\nimport librosa\nimport numpy as np\n\naudio_dir = f\"{DRIVE_DATA_DIR}/audio\"\nwav_files = sorted(glob.glob(f\"{audio_dir}/*.wav\"))\n\nprint(f\"Found {len(wav_files)} WAV files in {audio_dir}\")\nif len(wav_files) == 0:\n    raise RuntimeError(\"No WAV files found. Did you run upload_training_data.py?\")\n\nexamples = []\nskipped = 0\nfor wav_path in wav_files:\n    json_path = wav_path.replace(\".wav\", \".json\")\n    if not os.path.exists(json_path):\n        skipped += 1\n        continue\n    with open(json_path) as f:\n        meta = json.load(f)\n\n    # Load audio\n    audio, sr = librosa.load(wav_path, sr=16000, mono=True)\n    duration = len(audio) / 16000\n\n    if duration > MAX_AUDIO_SECS:\n        skipped += 1\n        continue\n\n    examples.append({\n        \"path\":         wav_path,\n        \"audio\":        audio,\n        \"duration\":     duration,\n        \"small_text\":   meta.get(\"text\", \"\"),\n        \"confidence\":   meta.get(\"confidence\", 1.0),\n        \"low_confidence\": meta.get(\"low_confidence\", False),\n        \"speaker_id\":   meta.get(\"speaker_id\"),\n    })\n\nprint(f\"Loaded: {len(examples)} examples  |  Skipped: {skipped}\")\n\n# Stats\nconfs = [e[\"confidence\"] for e in examples]\nlow_conf = [e for e in examples if e[\"confidence\"] < CONFIDENCE_THRESHOLD]\nprint(f\"Low-confidence (<{CONFIDENCE_THRESHOLD}): {len(low_conf)} / {len(examples)}\")\nprint(f\"Confidence range: {min(confs):.3f} – {max(confs):.3f}  mean: {np.mean(confs):.3f}\")\ndurations = [e[\"duration\"] for e in examples]\nprint(f\"Duration range: {min(durations):.1f}s – {max(durations):.1f}s  mean: {np.mean(durations):.1f}s\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# ── Step 4: Generate pseudo-labels with Whisper Large-v3 ───────────────────\n# Large-v3 is our \"oracle\". We run it on every example to get high-quality labels.\n# Training signal = cases where Small's prediction differs from Large's.\n#\n# NOTE: This is the slowest step. ~1-3s per utterance on T4.\n# For 500 utterances, expect ~15-25 minutes.\n\nimport torch\nfrom transformers import WhisperProcessor, WhisperForConditionalGeneration\n\nprint(f\"Loading {WHISPER_LARGE}...\")\nlarge_processor = WhisperProcessor.from_pretrained(WHISPER_LARGE)\nlarge_model = WhisperForConditionalGeneration.from_pretrained(\n    WHISPER_LARGE,\n    torch_dtype=torch.float16,\n    device_map=\"cuda\",\n)\nlarge_model.eval()\nprint(\"Large-v3 loaded ✓\")\n\ndef transcribe_large(audio: np.ndarray) -> str:\n    inputs = large_processor(\n        audio, sampling_rate=16000, return_tensors=\"pt\"\n    ).input_features.to(\"cuda\", dtype=torch.float16)\n    with torch.no_grad():\n        predicted_ids = large_model.generate(inputs, language=\"en\", task=\"transcribe\")\n    return large_processor.batch_decode(predicted_ids, skip_special_tokens=True)[0].strip()\n\nprint(f\"\\nRunning Large-v3 on {len(examples)} examples...\")\nfor i, ex in enumerate(examples):\n    ex[\"large_text\"] = transcribe_large(ex[\"audio\"])\n    if (i + 1) % 20 == 0:\n        print(f\"  {i+1}/{len(examples)}\")\n\n# Show divergence stats\nsame  = sum(1 for e in examples if e[\"large_text\"].lower() == e[\"small_text\"].lower())\ndiff  = len(examples) - same\nprint(f\"\\nLarge vs Small: {diff}/{len(examples)} differ ({100*diff/len(examples):.1f}%)\")\nprint(\"Sample divergences:\")\nfor ex in [e for e in examples if e[\"large_text\"].lower() != e[\"small_text\"].lower()][:5]:\n    print(f\"  Small: {ex['small_text']!r}\")\n    print(f\"  Large: {ex['large_text']!r}\")\n    print()\n\n# Free Large model memory before training\ndel large_model\ntorch.cuda.empty_cache()\nprint(\"Large-v3 unloaded — GPU memory freed for training.\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# ── Step 5: Prepare training dataset ────────────────────────────────────────\n# Training set = all examples with large_text as the label.\n# We use ALL examples (not just divergences) so Small learns the full distribution.\n# The divergences are where the learning signal is strongest.\n\nimport random\nfrom datasets import Dataset\nfrom transformers import WhisperFeatureExtractor, WhisperTokenizer\n\nfeature_extractor = WhisperFeatureExtractor.from_pretrained(WHISPER_SMALL)\ntokenizer = WhisperTokenizer.from_pretrained(WHISPER_SMALL, language=\"en\", task=\"transcribe\")\n\ndef prepare_example(ex: dict) -> dict:\n    # Audio → mel spectrogram features\n    input_features = feature_extractor(\n        ex[\"audio\"], sampling_rate=16000, return_tensors=\"np\"\n    ).input_features[0]\n    # Text → token ids (use Large's label as ground truth)\n    labels = tokenizer(ex[\"large_text\"]).input_ids\n    return {\n        \"input_features\": input_features,\n        \"labels\":         labels,\n        \"reference\":      ex[\"large_text\"],\n        \"small_pred\":     ex[\"small_text\"],\n    }\n\nprint(\"Preparing dataset...\")\nprepared = [prepare_example(ex) for ex in examples]\n\n# Train / val split (stratify by whether Small was correct)\nrandom.shuffle(prepared)\nn_val = max(1, int(len(prepared) * VALIDATION_SPLIT))\nval_data   = prepared[:n_val]\ntrain_data = prepared[n_val:]\n\ntrain_ds = Dataset.from_list(train_data)\nval_ds   = Dataset.from_list(val_data)\n\nprint(f\"Train: {len(train_ds)}  |  Val: {len(val_ds)}\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# ── Step 6: Data collator ───────────────────────────────────────────────────\n# Whisper requires padding input_features to 3000 frames and labels to max_length.\nimport torch\nfrom dataclasses import dataclass\nfrom typing import Any\n\n@dataclass\nclass WhisperDataCollator:\n    processor: Any\n    decoder_start_token_id: int\n\n    def __call__(self, features):\n        input_features = [{\"input_features\": f[\"input_features\"]} for f in features]\n        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n\n        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n\n        # Replace padding token id with -100 so loss ignores it\n        labels = labels_batch[\"input_ids\"].masked_fill(\n            labels_batch.attention_mask.ne(1), -100\n        )\n        # Remove bos token if present\n        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n            labels = labels[:, 1:]\n\n        batch[\"labels\"] = labels\n        return batch\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# ── Step 7: Load Whisper Small + apply LoRA ─────────────────────────────────\nfrom transformers import WhisperForConditionalGeneration, WhisperProcessor\nfrom peft import LoraConfig, get_peft_model\n\nprint(f\"Loading {WHISPER_SMALL}...\")\nprocessor = WhisperProcessor.from_pretrained(WHISPER_SMALL)\nmodel = WhisperForConditionalGeneration.from_pretrained(\n    WHISPER_SMALL,\n    torch_dtype=torch.float16,\n    device_map=\"cuda\",\n)\nmodel.config.forced_decoder_ids = None\nmodel.config.suppress_tokens = []\nmodel.generation_config.forced_decoder_ids = None\n\n# Apply LoRA to encoder query/value attention projections\nlora_config = LoraConfig(\n    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"SEQ_2_SEQ_LM\",\n)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\ndata_collator = WhisperDataCollator(\n    processor=processor,\n    decoder_start_token_id=model.config.decoder_start_token_id,\n)\nprint(\"Model ready ✓\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# ── Step 8: Define WER metric ───────────────────────────────────────────────\nimport evaluate\nimport numpy as np\n\nwer_metric = evaluate.load(\"wer\")\n\ndef compute_metrics(pred):\n    pred_ids   = pred.predictions\n    label_ids  = pred.label_ids\n\n    # Replace -100 with pad token id\n    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n\n    pred_str  = processor.tokenizer.batch_decode(pred_ids,  skip_special_tokens=True)\n    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n\n    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n    return {\"wer\": round(wer, 4)}\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# ── Step 9: Train ───────────────────────────────────────────────────────────\nfrom transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n\ncheckpoint_dir = f\"{DRIVE_MODELS_DIR}/checkpoints/{CYCLE_NAME}\"\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=checkpoint_dir,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n    learning_rate=LEARNING_RATE,\n    warmup_steps=max(10, len(train_ds) // 20),\n    num_train_epochs=TRAIN_EPOCHS,\n    fp16=True,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"wer\",\n    greater_is_better=False,\n    predict_with_generate=True,\n    generation_max_length=225,\n    logging_steps=10,\n    remove_unused_columns=False,\n    label_names=[\"labels\"],\n    report_to=\"none\",    # disable wandb\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=val_ds,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    tokenizer=processor.feature_extractor,\n)\n\nprint(f\"Starting training — {TRAIN_EPOCHS} epochs on {len(train_ds)} examples\")\nprint(f\"Checkpoints → {checkpoint_dir}\")\nprint()\ntrainer.train()\nprint(\"\\nTraining complete ✓\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# ── Step 10: Quick validation ───────────────────────────────────────────────\nprint(\"Validation set results:\")\nprint()\n\ncorrect = 0\nfor ex in val_data[:10]:    # spot-check first 10\n    audio = next(e[\"audio\"] for e in examples\n                 if e[\"large_text\"] == ex[\"reference\"])\n    inputs = processor(\n        audio, sampling_rate=16000, return_tensors=\"pt\"\n    ).input_features.to(\"cuda\", dtype=torch.float16)\n    with torch.no_grad():\n        pred_ids = model.generate(inputs, language=\"en\", task=\"transcribe\")\n    pred = processor.batch_decode(pred_ids, skip_special_tokens=True)[0].strip()\n\n    match = pred.lower() == ex[\"reference\"].lower()\n    correct += match\n    status = \"✓\" if match else \"✗\"\n    print(f\"  {status} REF: {ex['reference']!r}\")\n    if not match:\n        print(f\"    HYP: {pred!r}\")\n\nprint(f\"\\nSpot-check accuracy: {correct}/10\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# ── Step 11: Merge LoRA and save to Drive ───────────────────────────────────\n# Merge LoRA weights into the base model and save as a standard HuggingFace model.\n# This is what download_adapter.py will pick up and convert to MLX.\n\nmerged_dir = f\"{DRIVE_MODELS_DIR}/merged/{CYCLE_NAME}\"\n\nprint(f\"Merging LoRA weights...\")\nmerged_model = model.merge_and_unload()\nmerged_model = merged_model.to(torch.float32)   # convert from fp16 before saving\n\nprint(f\"Saving to Drive: {merged_dir}\")\nmerged_model.save_pretrained(merged_dir)\nprocessor.save_pretrained(merged_dir)\n\nprint(f\"\\nSaved ✓\")\nprint(f\"\\nNext steps on your local machine:\")\nprint(f\"  python scripts/download_adapter.py --cycle {CYCLE_NAME}\")\nprint(f\"  python scripts/evaluate_whisper.py --label {CYCLE_NAME}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Done — what to do next on your Mac\n\n```bash\n# 1. Download and convert to MLX\npython scripts/download_adapter.py --cycle cycle-1\n\n# 2. Evaluate — compare baseline vs this cycle\npython scripts/evaluate_whisper.py --label cycle-1\n\n# 3. Compare results\npython scripts/evaluate_whisper.py --compare \\\n    data/benchmark/evals/<baseline>.json \\\n    data/benchmark/evals/<cycle-1>.json\n```\n\nThe `download_adapter.py` script will automatically update `config/default.yaml` \nto point to the new MLX model. Restart the perception service to use it.\n\n**To run a second flywheel cycle:**\n1. Increment `CYCLE_NAME` to `cycle-2` in the config cell\n2. Collect more data with the updated model (it will generate different low-confidence samples)\n3. Re-run this notebook\n"
    }
  ]
}